import docx
from docx import Document
import mammoth
from spellchecker import SpellChecker
try:
    from hanspell import spell_checker
    HANSPELL_AVAILABLE = True
except ImportError:
    HANSPELL_AVAILABLE = False
    spell_checker = None

try:
    from pykospacing import Spacing
    PYKOSPACING_AVAILABLE = True
except ImportError:
    PYKOSPACING_AVAILABLE = False
    Spacing = None
import re
import time
import logging
from typing import Dict, List, Any
from collections import Counter
from config_manager import config

class DocumentAnalyzer:
    def __init__(self):
        # ì˜ì–´ ë§ì¶¤ë²• ê²€ì‚¬ê¸°
        self.spell_en = SpellChecker()
        # í•œêµ­ì–´ ë„ì–´ì“°ê¸° êµì •ê¸°
        self.spacing = Spacing() if PYKOSPACING_AVAILABLE else None
        # ì˜¤ë¥˜ ì¹´ìš´í„° ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ì´ˆê¸°í™”
        self._korean_error_count = 0
        self._korean_last_reset = time.time()
        self._korean_disabled_logged = False
        
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """ë¬¸ì„œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
        
        start_time = time.time()
        max_time = config.get("error_handling.max_analysis_time_seconds", 30)
        
        # ì „ì²´ ë¶„ì„ì´ ë¹„í™œì„±í™”ëœ ê²½ìš°
        if not config.get("analysis_settings.enabled", True):
            return {"message": "Document analysis is disabled", "recommendations": []}
        
        try:
            # ë¬¸ì„œ ì½ê¸°
            doc = Document(file_path)
        except Exception as e:
            if config.get("error_handling.log_errors", True):
                logging.error(f"Failed to read document: {e}")
            raise Exception(f"Failed to read document. Please ensure it's a valid .docx file: {str(e)}")
        
        # HTML ë³€í™˜ (mammoth ì‚¬ìš©)
        try:
            with open(file_path, "rb") as docx_file:
                result = mammoth.convert_to_html(docx_file)
                html_content = result.value
        except Exception as e:
            # HTML ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰ (ì„ íƒì  ê¸°ëŠ¥)
            html_content = ""
            if config.get("error_handling.log_errors", True):
                logging.warning(f"HTML conversion failed: {e}")
        
        # ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        analysis_result = {
            "filename": file_path.split('/')[-1].split('\\')[-1],
            "syntax_errors": [],
            "spelling_errors": {"korean": [], "english": [], "spacing": []},
            "style_consistency": {},
            "document_structure": {},
            "statistics": {},
            "recommendations": [],
            "analysis_config": {
                "syntax_enabled": config.is_enabled("syntax_errors"),
                "spelling_enabled": config.is_enabled("spelling_errors"),
                "style_enabled": config.is_enabled("style_consistency"),
                "structure_enabled": config.is_enabled("document_structure")
            }
        }
        
        # ê° ë¶„ì„ ë‹¨ê³„ë³„ ì‹¤í–‰ (ì„¤ì •ì— ë”°ë¼)
        try:
            if config.is_enabled("syntax_errors"):
                analysis_result["syntax_errors"] = self._check_syntax_errors_with_config(doc)
            
            if config.is_enabled("spelling_errors"):
                analysis_result["spelling_errors"] = self._check_spelling_errors_with_config(doc)
            
            if config.is_enabled("style_consistency"):
                analysis_result["style_consistency"] = self._check_style_consistency_with_config(doc)
            
            if config.is_enabled("document_structure"):
                analysis_result["document_structure"] = self._analyze_document_structure_with_config(doc)
            
            # í†µê³„ëŠ” í•­ìƒ ìƒì„±
            analysis_result["statistics"] = self._get_document_statistics(doc)
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            if config.is_enabled("recommendations"):
                analysis_result["recommendations"] = self._generate_recommendations_with_config(analysis_result)
            
        except Exception as e:
            if config.get("error_handling.continue_on_error", True):
                if config.get("error_handling.log_errors", True):
                    logging.error(f"Analysis error: {e}")
                analysis_result["error"] = str(e)
            else:
                raise e
        
        # ë¶„ì„ ì‹œê°„ ì²´í¬
        elapsed_time = time.time() - start_time
        analysis_result["analysis_time"] = elapsed_time
        
        if elapsed_time > max_time:
            logging.warning(f"Analysis took {elapsed_time:.2f}s, exceeding limit of {max_time}s")
        
        return analysis_result
    
    def _check_syntax_errors_with_config(self, doc: Document) -> List[Dict[str, Any]]:
        """ì„¤ì • ê¸°ë°˜ êµ¬ë¬¸ ì˜¤ë¥˜ ê²€ì‚¬"""
        errors = []
        max_errors_per_paragraph = config.get_max_errors_per_paragraph("syntax_errors")
        ignore_short = config.should_ignore_short_paragraphs()
        min_length = config.get_min_paragraph_length()
        ignore_patterns = config.get_ignore_patterns("syntax_errors")
        
        # ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´ë“¤
        compiled_patterns = []
        for pattern in ignore_patterns:
            try:
                compiled_patterns.append(re.compile(pattern))
            except re.error:
                logging.warning(f"Invalid regex pattern: {pattern}")
        
        for i, paragraph in enumerate(doc.paragraphs):
            text = paragraph.text.strip()
            if not text:
                continue
            
            # ì§§ì€ ë¬¸ë‹¨ ë¬´ì‹œ ì„¤ì • í™•ì¸
            if ignore_short and len(text) < min_length:
                continue
            
            # ë¬´ì‹œ íŒ¨í„´ í™•ì¸
            should_ignore = False
            for pattern in compiled_patterns:
                if pattern.search(text):
                    should_ignore = True
                    break
            
            if should_ignore:
                continue
            
            paragraph_errors = []
            
            # ê° ì˜¤ë¥˜ ìœ í˜•ë³„ ê²€ì‚¬ (ì„¤ì •ì— ë”°ë¼)
            if config.get("syntax_errors.check_punctuation", True):
                paragraph_errors.extend(self._check_punctuation_errors(text, i + 1))
            
            if config.get("syntax_errors.check_brackets", True):
                paragraph_errors.extend(self._check_bracket_errors(text, i + 1))
            
            if config.get("syntax_errors.check_quotes", True):
                paragraph_errors.extend(self._check_quote_errors(text, i + 1))
            
            # ë¬¸ë‹¨ë‹¹ ìµœëŒ€ ì˜¤ë¥˜ ìˆ˜ ì œí•œ
            if len(paragraph_errors) > max_errors_per_paragraph:
                paragraph_errors = paragraph_errors[:max_errors_per_paragraph]
                paragraph_errors.append({
                    "type": "too_many_errors",
                    "paragraph": i + 1,
                    "text": text[:100] + "..." if len(text) > 100 else text,
                    "description": f"ì´ ë¬¸ë‹¨ì—ì„œ {len(paragraph_errors)}ê°œ ì´ìƒì˜ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì–´ ì¼ë¶€ë§Œ í‘œì‹œí•©ë‹ˆë‹¤."
                })
            
            errors.extend(paragraph_errors)
        
        return errors
    
    def _check_punctuation_errors(self, text: str, paragraph_num: int) -> List[Dict[str, Any]]:
        """ë¬¸ì¥ ë¶€í˜¸ ì˜¤ë¥˜ ê²€ì‚¬"""
        errors = []
        
        # ë¬¸ì¥ ì‹œì‘ ì‹œ ëŒ€ë¬¸ì ì‚¬ìš© ê¶Œì¥ ì²´í¬ (ì„¤ì •ìœ¼ë¡œ ì œì–´)
        if config.get("syntax_errors.check_punctuation_capitalization", False):
            if re.search(r'[.!?]\s*[a-zê°€-í£]', text):
                errors.append({
                    "type": "punctuation_capitalization",
                    "paragraph": paragraph_num,
                    "text": text[:100] + "..." if len(text) > 100 else text,
                    "description": "ë¬¸ì¥ ì‹œì‘ ì‹œ ëŒ€ë¬¸ì ì‚¬ìš© ê¶Œì¥"
                })
        
        # ë‹¤ë¥¸ ë¬¸ì¥ ë¶€í˜¸ ì˜¤ë¥˜ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        # ì˜ˆ: ì—°ì†ëœ ë¬¸ì¥ë¶€í˜¸, ì˜ëª»ëœ ë¬¸ì¥ë¶€í˜¸ ì‚¬ìš© ë“±
        
        return errors
    
    def _check_bracket_errors(self, text: str, paragraph_num: int) -> List[Dict[str, Any]]:
        """ê´„í˜¸ ë¶ˆì¼ì¹˜ ê²€ì‚¬"""
        errors = []
        open_brackets = text.count('(')
        close_brackets = text.count(')')
        if open_brackets != close_brackets:
            errors.append({
                "type": "brackets",
                "paragraph": paragraph_num,
                "text": text[:100] + "..." if len(text) > 100 else text,
                "description": f"ê´„í˜¸ ë¶ˆì¼ì¹˜ (ì—´ë¦¼: {open_brackets}, ë‹«í˜: {close_brackets})"
            })
        return errors
    
    def _check_quote_errors(self, text: str, paragraph_num: int) -> List[Dict[str, Any]]:
        """ë”°ì˜´í‘œ ë¶ˆì¼ì¹˜ ê²€ì‚¬"""
        errors = []
        quote_count = text.count('"')
        if quote_count % 2 != 0:
            errors.append({
                "type": "quotes",
                "paragraph": paragraph_num,
                "text": text[:100] + "..." if len(text) > 100 else text,
                "description": "ë”°ì˜´í‘œ ë¶ˆì¼ì¹˜"
            })
        return errors

    def _check_spelling_errors_with_config(self, doc: Document) -> Dict[str, List[Dict[str, Any]]]:
        """ì„¤ì • ê¸°ë°˜ ë§ì¶¤ë²• ì˜¤ë¥˜ ê²€ì‚¬"""
        korean_errors = []
        english_errors = []
        spacing_errors = []
        
        ignore_words = set(config.get_ignore_words())
        korean_max = config.get("spelling_errors.korean_max_errors_per_paragraph", 5)
        english_max = config.get("spelling_errors.english_max_errors_per_paragraph", 3)
        spacing_max = config.get("spelling_errors.spacing_max_errors_per_paragraph", 2)
        min_word_length = config.get("spelling_errors.english_min_word_length", 3)
        
        for i, paragraph in enumerate(doc.paragraphs):
            text = paragraph.text.strip()
            if not text:
                continue
            
            paragraph_korean_errors = []
            paragraph_english_errors = []
            paragraph_spacing_errors = []
            
            # í•œêµ­ì–´ ë§ì¶¤ë²• ê²€ì‚¬
            if config.get("spelling_errors.korean_enabled", True) and HANSPELL_AVAILABLE:
                # ì„¤ì •ê°’ ë¡œë“œ
                max_text_length = config.get("spelling_errors.korean_max_text_length", 500)
                min_korean_chars = config.get("spelling_errors.korean_min_korean_chars", 5)
                api_delay = config.get("spelling_errors.korean_api_delay", 0.1)
                skip_patterns = config.get("spelling_errors.korean_skip_patterns", [])
                
                # í•œêµ­ì–´ ê¸€ì ìˆ˜ í™•ì¸
                korean_char_count = len(re.findall(r'[ê°€-í£]', text))
                korean_check_enabled = korean_char_count >= min_korean_chars
                
                # ì œì™¸ íŒ¨í„´ í™•ì¸
                if korean_check_enabled:
                    for pattern in skip_patterns:
                        try:
                            if re.match(pattern, text):
                                korean_check_enabled = False
                                break
                        except re.error:
                            continue
                
                # ì—°ì† ì˜¤ë¥˜ ì²´í¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
                if not hasattr(self, '_korean_error_count'):
                    self._korean_error_count = 0
                if not hasattr(self, '_korean_last_reset'):
                    self._korean_last_reset = time.time()
                
                max_consecutive_errors = config.get("spelling_errors.korean_max_consecutive_errors", 10)
                
                # 5ë¶„ë§ˆë‹¤ ì˜¤ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹ (ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜)
                current_time = time.time()
                if current_time - self._korean_last_reset > 300:  # 5ë¶„
                    if self._korean_error_count > 0:
                        self._korean_error_count = 0
                        self._korean_last_reset = current_time
                        self._korean_disabled_logged = False  # ë¡œê·¸ í”Œë˜ê·¸ ë¦¬ì…‹
                        if config.get("error_handling.log_errors", True):
                            logging.info("Korean spell check error counter reset - re-enabling Korean spell check")
                
                # í•œêµ­ì–´ ê²€ì‚¬ ì‹¤í–‰
                if korean_check_enabled and self._korean_error_count < max_consecutive_errors:
                    try:
                        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                        if len(text) > max_text_length:
                            text_chunks = [text[i:i+max_text_length] for i in range(0, len(text), max_text_length)]
                        else:
                            text_chunks = [text]
                        
                        chunk_success = False
                        for chunk_idx, chunk in enumerate(text_chunks):
                            try:
                                spelled_sent = spell_checker.check(chunk)
                                chunk_success = True
                                
                                # ì„±ê³µì ì¸ í˜¸ì¶œ ì‹œ ì˜¤ë¥˜ ì¹´ìš´íŠ¸ ê°ì†Œ (ì ì§„ì  ë³µêµ¬)
                                if self._korean_error_count > 0:
                                    self._korean_error_count = max(0, self._korean_error_count - 1)
                                
                                # ê²°ê³¼ ì²˜ë¦¬
                                if hasattr(spelled_sent, 'errors') and spelled_sent.errors:
                                    for error in spelled_sent.errors[:korean_max]:
                                        if isinstance(error, (list, tuple)) and len(error) >= 2:
                                            paragraph_korean_errors.append({
                                                "paragraph": i + 1,
                                                "original": str(error[0]),
                                                "suggestion": str(error[1]),
                                                "context": text[:100] + "..." if len(text) > 100 else text
                                            })
                                elif hasattr(spelled_sent, 'checked') and spelled_sent.checked != chunk:
                                    paragraph_korean_errors.append({
                                        "paragraph": i + 1,
                                        "original": chunk,
                                        "suggestion": spelled_sent.checked,
                                        "context": text[:100] + "..." if len(text) > 100 else text
                                    })
                                
                                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                                if chunk_idx < len(text_chunks) - 1:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                                    time.sleep(api_delay)
                                    
                            except Exception as chunk_error:
                                self._korean_error_count += 1
                                if config.get("error_handling.log_errors", True):
                                    logging.debug(f"Korean spell check failed for chunk in paragraph {i+1}: {str(chunk_error)}")
                                break  # ì²­í¬ ì²˜ë¦¬ ì¤‘ë‹¨
                        
                        # ì „ì²´ ì²­í¬ ì²˜ë¦¬ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ê²½ê³ 
                        if not chunk_success:
                            if config.get("error_handling.log_errors", True):
                                logging.debug(f"Korean spell check completely failed for paragraph {i+1}")
                            
                    except Exception as e:
                        self._korean_error_count += 1
                        error_msg = str(e)
                        if config.get("error_handling.log_errors", True):
                            # 'result' ì˜¤ë¥˜ëŠ” ë””ë²„ê·¸ ë ˆë²¨ë¡œ, ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ê²½ê³  ë ˆë²¨ë¡œ
                            if "result" in error_msg.lower() or "timeout" in error_msg.lower():
                                logging.debug(f"Korean spell check API issue for paragraph {i+1}: {error_msg}")
                            else:
                                logging.debug(f"Korean spell check failed for paragraph {i+1}: {error_msg}")
                
                elif self._korean_error_count >= max_consecutive_errors:
                    # í•œ ë²ˆë§Œ ë¡œê·¸ ì¶œë ¥í•˜ë„ë¡ ê°œì„ 
                    if not hasattr(self, '_korean_disabled_logged') or not self._korean_disabled_logged:
                        if config.get("error_handling.log_errors", True):
                            logging.warning(f"Korean spell check temporarily disabled due to {self._korean_error_count} consecutive errors. Will retry in 5 minutes.")
                        self._korean_disabled_logged = True
            
            # ì˜ì–´ ë§ì¶¤ë²• ê²€ì‚¬
            if config.get("spelling_errors.english_enabled", True):
                try:
                    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
                    # ê¸¸ì´ í•„í„°ë§ ë° ë¬´ì‹œ ë‹¨ì–´ ì œì™¸
                    english_words = [w for w in english_words 
                                   if len(w) >= min_word_length and w.lower() not in ignore_words]
                    
                    misspelled = self.spell_en.unknown(english_words)
                    
                    for word in list(misspelled)[:english_max]:
                        if word.lower() not in ignore_words:
                            candidates = self.spell_en.candidates(word)
                            suggestions = list(candidates)[:3] if candidates else []
                            paragraph_english_errors.append({
                                "paragraph": i + 1,
                                "word": word,
                                "suggestions": suggestions,
                                "context": text[:100] + "..." if len(text) > 100 else text
                            })
                except Exception as e:
                    if config.get("error_handling.log_errors", True):
                        logging.warning(f"English spell check failed for paragraph {i+1}: {e}")
            
            # í•œêµ­ì–´ ë„ì–´ì“°ê¸° ê²€ì‚¬
            if config.get("spelling_errors.spacing_enabled", True) and PYKOSPACING_AVAILABLE and self.spacing:
                try:
                    corrected = self.spacing(text)
                    if corrected != text:
                        paragraph_spacing_errors.append({
                            "paragraph": i + 1,
                            "original": text,
                            "corrected": corrected,
                            "context": text[:100] + "..." if len(text) > 100 else text
                        })
                        
                        if len(paragraph_spacing_errors) >= spacing_max:
                            break
                except Exception as e:
                    if config.get("error_handling.log_errors", True):
                        logging.warning(f"Spacing check failed for paragraph {i+1}: {e}")
            
            korean_errors.extend(paragraph_korean_errors)
            english_errors.extend(paragraph_english_errors)
            spacing_errors.extend(paragraph_spacing_errors)
        
        return {
            "korean": korean_errors,
            "english": english_errors,
            "spacing": spacing_errors
        }
    
    def _check_style_consistency_with_config(self, doc: Document) -> Dict[str, Any]:
        """ì„¤ì • ê¸°ë°˜ ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ê²€ì‚¬"""
        result = {
            "headings": [],
            "heading_styles": {},
            "font_consistency": {},
            "toc_structure": {"valid": True, "issues": []}
        }
        
        try:
            # ì œëª© ìŠ¤íƒ€ì¼ ë¶„ì„
            if config.get("style_consistency.heading_structure_enabled", True):
                headings = []
                heading_styles = Counter()
                
                for paragraph in doc.paragraphs:
                    if paragraph.style.name.startswith('Heading'):
                        headings.append({
                            "text": paragraph.text,
                            "style": paragraph.style.name,
                            "level": int(paragraph.style.name.split()[-1]) if paragraph.style.name.split()[-1].isdigit() else 1
                        })
                        heading_styles[paragraph.style.name] += 1
                
                result["headings"] = headings
                result["heading_styles"] = dict(heading_styles)
                result["toc_structure"] = self._analyze_toc_structure_with_config(headings)
            
            # í°íŠ¸ ì¼ê´€ì„± ê²€ì‚¬
            if config.get("style_consistency.font_consistency_enabled", True):
                font_usage = Counter()
                font_size_usage = Counter()
                min_usage = config.get("style_consistency.min_font_usage_threshold", 2)
                
                for paragraph in doc.paragraphs:
                    for run in paragraph.runs:
                        if run.font and run.font.name:
                            font_usage[run.font.name] += 1
                        if run.font and run.font.size:
                            font_size_usage[str(run.font.size)] += 1
                
                # ìµœì†Œ ì‚¬ìš© íšŸìˆ˜ ì´ìƒì¸ í°íŠ¸ë§Œ í¬í•¨
                filtered_fonts = {k: v for k, v in font_usage.items() if v >= min_usage}
                filtered_sizes = {k: v for k, v in font_size_usage.items() if v >= min_usage}
                
                result["font_consistency"] = {
                    "fonts_used": filtered_fonts,
                    "font_sizes_used": filtered_sizes,
                    "main_font": font_usage.most_common(1)[0][0] if font_usage else None,
                    "main_font_size": font_size_usage.most_common(1)[0][0] if font_size_usage else None
                }
        
        except Exception as e:
            if config.get("error_handling.log_errors", True):
                logging.error(f"Style consistency check failed: {e}")
            if not config.get("error_handling.continue_on_error", True):
                raise e
        
        return result
    
    def _analyze_toc_structure_with_config(self, headings: List[Dict]) -> Dict[str, Any]:
        """ì„¤ì • ê¸°ë°˜ ëª©ì°¨ êµ¬ì¡° ë¶„ì„"""
        if not headings:
            return {"valid": True, "issues": []}
        
        issues = []
        prev_level = 0
        max_skip = config.get("style_consistency.max_heading_level_skip", 1)
        allow_empty = config.get("style_consistency.allow_empty_headings", False)
        
        for i, heading in enumerate(headings):
            current_level = heading["level"]
            
            # ì œëª© ë ˆë²¨ ê±´ë„ˆë›°ê¸° ê²€ì‚¬
            if current_level > prev_level + max_skip:
                issues.append({
                    "type": "level_skip",
                    "heading": heading["text"],
                    "description": f"ì œëª© ë ˆë²¨ì´ {prev_level}ì—ì„œ {current_level}ë¡œ ë„ˆë¬´ ë§ì´ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤."
                })
            
            # ë¹ˆ ì œëª© ê²€ì‚¬
            if not allow_empty and not heading["text"].strip():
                issues.append({
                    "type": "empty_heading",
                    "heading": f"ì œëª© {i+1}",
                    "description": "ë¹ˆ ì œëª©ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                })
            
            prev_level = current_level
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "total_headings": len(headings),
            "max_level": max([h["level"] for h in headings]) if headings else 0
        }
    
    def _analyze_document_structure_with_config(self, doc: Document) -> Dict[str, Any]:
        """ì„¤ì • ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        total_paragraphs = len(doc.paragraphs)
        
        # ë‹¨ì¼ ë¼ì¸ ë¬¸ë‹¨ ë¬´ì‹œ ì„¤ì •
        if config.get("document_structure.ignore_single_line_paragraphs", True):
            non_empty_paragraphs = len([p for p in doc.paragraphs 
                                      if p.text.strip() and len(p.text.strip().split('\n')) > 1])
        else:
            non_empty_paragraphs = len([p for p in doc.paragraphs if p.text.strip()])
        
        empty_paragraphs = total_paragraphs - non_empty_paragraphs
        
        result = {
            "total_paragraphs": total_paragraphs,
            "non_empty_paragraphs": non_empty_paragraphs,
            "empty_paragraphs": empty_paragraphs,
            "table_count": 0,
            "estimated_image_count": 0
        }
        
        # ìµœì†Œ ë¬¸ë‹¨ ìˆ˜ í™•ì¸
        min_paragraphs = config.get("document_structure.min_paragraphs_for_analysis", 5)
        if total_paragraphs < min_paragraphs:
            result["warning"] = f"ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ {min_paragraphs}ê°œ ë¬¸ë‹¨ ê¶Œì¥)"
        
        # í‘œ ë¶„ì„
        if config.get("document_structure.analyze_tables", True):
            result["table_count"] = len(doc.tables)
        
        # ì´ë¯¸ì§€ ë¶„ì„
        if config.get("document_structure.analyze_images", True):
            try:
                image_count = 0
                for paragraph in doc.paragraphs:
                    for run in paragraph.runs:
                        if hasattr(run, 'element') and run.element.xpath('.//a:blip'):
                            image_count += 1
                result["estimated_image_count"] = image_count
            except Exception as e:
                if config.get("error_handling.log_errors", True):
                    logging.warning(f"Image analysis failed: {e}")
                result["estimated_image_count"] = 0
        
        return result
    
    def _generate_recommendations_with_config(self, analysis_result: Dict[str, Any]) -> List[str]:
        """ì„¤ì • ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        max_recommendations = config.get("recommendations.max_recommendations", 10)
        priority_order = config.get("recommendations.priority_order", 
                                  ["syntax_errors", "spelling_errors", "style_consistency", "document_structure"])
        severity_thresholds = config.get("recommendations.severity_thresholds", 
                                       {"critical": 10, "warning": 5, "info": 1})
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¶”ì²œì‚¬í•­ ìƒì„±
        for section in priority_order:
            if len(recommendations) >= max_recommendations:
                break
                
            if section == "syntax_errors" and analysis_result.get("syntax_errors"):
                error_count = len(analysis_result["syntax_errors"])
                if error_count >= severity_thresholds.get("critical", 10):
                    recommendations.append(f"ğŸš¨ ì‹¬ê°: êµ¬ë¬¸ ì˜¤ë¥˜ {error_count}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                elif error_count >= severity_thresholds.get("warning", 5):
                    recommendations.append(f"âš ï¸ ê²½ê³ : êµ¬ë¬¸ ì˜¤ë¥˜ {error_count}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
                else:
                    recommendations.append(f"â„¹ï¸ ì •ë³´: êµ¬ë¬¸ ì˜¤ë¥˜ {error_count}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            elif section == "spelling_errors":
                spelling_data = analysis_result.get("spelling_errors", {})
                total_spelling_errors = (
                    len(spelling_data.get("korean", [])) +
                    len(spelling_data.get("english", [])) +
                    len(spelling_data.get("spacing", []))
                )
                if total_spelling_errors > 0:
                    if total_spelling_errors >= severity_thresholds.get("critical", 10):
                        recommendations.append(f"ğŸš¨ ì‹¬ê°: ë§ì¶¤ë²• ì˜¤ë¥˜ {total_spelling_errors}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    elif total_spelling_errors >= severity_thresholds.get("warning", 5):
                        recommendations.append(f"âš ï¸ ê²½ê³ : ë§ì¶¤ë²• ì˜¤ë¥˜ {total_spelling_errors}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        recommendations.append(f"â„¹ï¸ ì •ë³´: ë§ì¶¤ë²• ì˜¤ë¥˜ {total_spelling_errors}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            elif section == "style_consistency":
                style_info = analysis_result.get("style_consistency", {})
                font_info = style_info.get("font_consistency", {})
                max_fonts = config.get("style_consistency.max_different_fonts", 3)
                
                if len(font_info.get("fonts_used", {})) > max_fonts:
                    recommendations.append(f"âš ï¸ ìŠ¤íƒ€ì¼: ì‚¬ìš©ëœ í°íŠ¸ê°€ {max_fonts}ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì¼ê´€ì„±ì„ ìœ„í•´ ì œí•œì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                
                if not style_info.get("toc_structure", {}).get("valid", True):
                    recommendations.append("âš ï¸ êµ¬ì¡°: ëª©ì°¨ êµ¬ì¡°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì œëª© ë ˆë²¨ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            elif section == "document_structure":
                structure = analysis_result.get("document_structure", {})
                max_empty_ratio = config.get("document_structure.max_empty_paragraph_ratio", 0.4)
                
                if structure.get("total_paragraphs", 0) > 0:
                    empty_ratio = structure.get("empty_paragraphs", 0) / structure["total_paragraphs"]
                    if empty_ratio > max_empty_ratio:
                        recommendations.append(f"âš ï¸ êµ¬ì¡°: ë¹ˆ ë¬¸ë‹¨ ë¹„ìœ¨ì´ {empty_ratio*100:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤. ì •ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ì¶”ì²œì‚¬í•­ì´ ì—†ìœ¼ë©´ ê¸ì •ì  ë©”ì‹œì§€
        if not recommendations:
            recommendations.append("âœ… ë¬¸ì„œê°€ ì „ë°˜ì ìœ¼ë¡œ ì˜ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        return recommendations[:max_recommendations]

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
    def _get_document_statistics(self, doc: Document) -> Dict[str, Any]:
        """ë¬¸ì„œ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        total_text = ""
        for paragraph in doc.paragraphs:
            total_text += paragraph.text + " "
        
        # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        korean_chars = len(re.findall(r'[ê°€-í£]', total_text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', total_text))
        total_chars = len(total_text.replace(' ', ''))
        
        # ë¬¸ì¥ ìˆ˜ ê³„ì‚°
        sentences = len(re.findall(r'[.!?]+', total_text))
        
        return {
            "korean_characters": korean_chars,
            "english_words": english_words,
            "total_characters": total_chars,
            "sentences": sentences,
            "estimated_reading_time_minutes": max(1, total_chars // 500)  # ë¶„ë‹¹ 500ì ê¸°ì¤€
        }